{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "The goal of this notebook is to prepare our data for the project. There are two sources of data that we are considering. One is the tabular data from the tracking data. We reformat this to be in such a way that one row represents a play. We make a huge assumption that we are only considering the offense here. The second form of data will be trying to use computer vision from the same data set. Hopefully this is able to capture the defense part.\n",
    "\n",
    "### Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing The Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The play data \n",
    "play_data = pd.read_csv(\"../Data/plays.csv\")\n",
    "\n",
    "# The player data\n",
    "player_data = pd.read_csv(\"../Data/players.csv\")\n",
    "\n",
    "# player scores \n",
    "player_scores = pd.read_csv(\"../Data/robust_player_scores.csv\")\n",
    "\n",
    "training_data_total = pd.read_csv(\"../Data/training_data_total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need a function to do this entire process\n",
    "\n",
    "def clean_week_data(week_data = None, play_data = None, player_data = None, player_scores = None):\n",
    "    # first filter out the frames that to just before and during the snap\n",
    "    pre_snap_data = week_data.loc[(week_data['displayName'] != 'football') & (week_data['frameType'] != 'AFTER_SNAP'),:]\n",
    "\n",
    "    # we then drop the columns that we definately do not need\n",
    "    pre_snap_data.drop(columns = ['displayName','frameType', 'time', 'jerseyNumber', 'event'], inplace = True)\n",
    "\n",
    "    # we calculate the distance moved from the previous frame\n",
    "    pre_snap_data[\"distance\"] =  np.sqrt(\n",
    "        pre_snap_data.groupby(['gameId', 'playId', 'nflId'])['x'].diff().abs()**2 + \n",
    "        pre_snap_data.groupby(['gameId', 'playId', 'nflId'])['y'].diff().abs()**2\n",
    "        )\n",
    "    \n",
    "    # fill in the na with a 0 for the first rolling \n",
    "    pre_snap_data['distance'] = pre_snap_data['distance'].fillna(0)\n",
    "\n",
    "    # we then get the total distance moved in the play\n",
    "    pre_snap_data['total_distance'] = pre_snap_data.groupby(['gameId', 'playId', 'nflId'])['distance'].transform('sum')\n",
    "\n",
    "    # drop the intermediate column from the data\n",
    "    pre_snap_data.drop(columns=['distance'], inplace = True)\n",
    "\n",
    "    # adding the player scores to the data \n",
    "    pre_snap_data_with_scores = pre_snap_data.merge(player_scores, on='nflId', how='left')\n",
    "    # now we add more features to each of plays from the play data\n",
    "    filtered_play_data = play_data.loc[:,[\"gameId\",\"playId\",\"down\",\"yardsToGo\",\"possessionTeam\",\"yardsGained\"]]\n",
    "\n",
    "    # merge this with our data \n",
    "    pre_snap_data_with_play_data = pre_snap_data_with_scores.merge(filtered_play_data,on = ['gameId','playId'], how = 'left')\n",
    "\n",
    "    # grab only the offensive teams\n",
    "    pre_snap_data_offense_only = pre_snap_data_with_play_data.loc[pre_snap_data_with_play_data['club'] == pre_snap_data_with_play_data['possessionTeam'], :]\n",
    "\n",
    "    # drop the column \n",
    "    pre_snap_data_offense_only.drop(columns = ['possessionTeam'], inplace = True)\n",
    "\n",
    "    # add the player position to the data\n",
    "    positions = player_data.loc[:,['nflId', 'position']]\n",
    "    pre_snap_data_with_positions = pre_snap_data_offense_only.merge(positions, on = 'nflId', how = 'left')\n",
    "\n",
    "    # drop the frame ID \n",
    "    # pre_snap_data_with_positions.drop(columns = ['frameId'], inplace = True)\n",
    "\n",
    "    # now we will reformat the data to have one obervation be one play\n",
    "    # this is the trickiest part\n",
    "    # we will then be ready to try this function with every week to construct our final data set\n",
    "\n",
    "    # each play will have 11 players\n",
    "    # for each player track a unique index for every play \n",
    "    pre_snap_data_with_positions['player_index'] = pre_snap_data_with_positions.groupby(['gameId', 'playId'])['nflId'].rank(method='dense').astype(int)\n",
    "    # pre_snap_data_with_positions['player_index'] = pre_snap_data_with_positions.groupby(['gameId', 'playId'])['position'].rank(method='dense').astype(int)\n",
    "\n",
    "\n",
    "    # track the columns that will habe the same value throughout the entire play\n",
    "    common_columns = [\"gameId\", \"playId\", \"club\", \"playDirection\", \"down\", \"yardsToGo\", \"yardsGained\"]\n",
    "\n",
    "    # all of the other colunms \n",
    "    different_columns = [\"gameId\", \"playId\", \"player_index\", \"x\", \"y\",\n",
    "                         \"s\", \"a\", \"dis\", \"o\", \"dir\", \"total_distance\", \"position\", \"player_score\"]\n",
    "    \n",
    "    # only keep the unqiue values for the common data\n",
    "    common_data = pre_snap_data_with_positions[common_columns].drop_duplicates()\n",
    "\n",
    "    # only keep the last frameId values \n",
    "    last_frame_data = (\n",
    "        pre_snap_data_with_positions.groupby(['gameId','playId','nflId']).tail(1)\n",
    "    )\n",
    "\n",
    "    # pivot the data so that each of the rows will be one play\n",
    "    pivoted_data = last_frame_data[different_columns].pivot(index=['gameId', 'playId'], columns='player_index').reset_index()\n",
    "\n",
    "    # some renaming for the new data frame\n",
    "    pivoted_data.columns = ['_'.join(map(str,col)).strip('_') for col in pivoted_data.columns]\n",
    "\n",
    "    pre_snap_data_reformated = pd.merge(common_data, pivoted_data, on = ['gameId', 'playId'])\n",
    "\n",
    "    # now for adding our counter factuals to the data set\n",
    "    #lets first drop all the ones that are missing\n",
    "    # we will then combine some of the plaued\n",
    "    # notice that all the values where the coverage is missing is due to the offense kneeling\n",
    "    # decide to drop sense theere is not much we can really do\n",
    "    non_kneeling_play_data = play_data.loc[play_data[\"pff_passCoverage\"].isna() != True,:]\n",
    "\n",
    "    label_mapping = {\n",
    "    'Cover-0': 'Man',\n",
    "    'Cover-1': 'Man',\n",
    "    'Cover-1 Double': 'Man',\n",
    "    'Cover-2': 'Zone',\n",
    "    '2-Man': 'Man',\n",
    "    'Cover-3': 'Zone',\n",
    "    'Cover-3 Seam': 'Zone',\n",
    "    'Cover-3 Cloud Right': 'Zone',\n",
    "    'Cover-3 Cloud Left': 'Zone',\n",
    "    'Cover-3 Double Cloud': 'Zone',\n",
    "    'Cover 6-Left': 'Zone',\n",
    "    'Cover-6 Right': 'Zone',\n",
    "    'Quarters': 'spec',\n",
    "    'Red Zone': 'spec',\n",
    "    'Goal Line': 'spec',\n",
    "    'Bracket': 'Zone',\n",
    "    'Prevent': 'Zone',\n",
    "    'Miscellaneous': 'spec'\n",
    "    }\n",
    "\n",
    "    non_kneeling_play_data['assigned_defense'] = non_kneeling_play_data['pff_passCoverage'].map(label_mapping)\n",
    "\n",
    "    # only keep the columns that are necessary to keep\n",
    "    coverage_assignments =  non_kneeling_play_data.loc[:,[\"gameId\", \"playId\", \"assigned_defense\"]]\n",
    "\n",
    "    # merge with the other data\n",
    "    final_week_data = pre_snap_data_reformated.merge(coverage_assignments, on = ['gameId','playId'], how = 'left')\n",
    "\n",
    "    print(f\"The total final data rows: {final_week_data.shape}\")\n",
    "\n",
    "\n",
    "    # drop the rows with no coverage assignments\n",
    "    return final_week_data.dropna()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Week 1\n",
      "The total final data rows: (1952, 118)\n",
      "Running Week 2\n",
      "The total final data rows: (1809, 118)\n",
      "Running Week 3\n",
      "The total final data rows: (1959, 118)\n",
      "Running Week 4\n",
      "The total final data rows: (1830, 118)\n",
      "Running Week 5\n",
      "The total final data rows: (1906, 118)\n",
      "Running Week 6\n",
      "The total final data rows: (1697, 118)\n",
      "Running Week 7\n",
      "The total final data rows: (1665, 118)\n",
      "Running Week 8\n",
      "The total final data rows: (1771, 118)\n",
      "Running Week 9\n",
      "The total final data rows: (1535, 118)\n"
     ]
    }
   ],
   "source": [
    "# we run all the data through the function and process it as one data frame \n",
    "total_training_data = []\n",
    "for i in range(9):\n",
    "    print(f'Running Week {i+1}')\n",
    "    week = clean_week_data(week_data = pd.read_csv(f\"../Data/tracking_week_{i+1}.csv\"), play_data = play_data, \n",
    "                           player_data = player_data, player_scores = player_scores)\n",
    "    # append to the list\n",
    "    total_training_data.append(week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to concatenate all of the training data into one data set\n",
    "training_data = pd.concat(total_training_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the data\n",
    "# training_data.to_csv('../Data/training_data_total.csv', index=False)\n",
    "# test_data.to_csv('../Data/test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Different Data\n",
    "We will now try only keep the top 8-10 teams seince we ran into poor policy tree from using too many categorical features. We will choose this from the rankings from the previous year that the data was collected at. We could not use DAL or SF since these teams did not play in week 9. \n",
    "\n",
    "Top Ten Teams\n",
    "1. KC\n",
    "2. GB\n",
    "3. TB\n",
    "4. BUF\n",
    "5. LA\n",
    "6. NE\n",
    "7. TEN\n",
    "8.  IND\n",
    "9.  ARI\n",
    "10. CIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the top ten team names\n",
    "top_ten = ['KC','GB','TB','BUF','LA','CIN','NE','TEN','ARI','IND']\n",
    "\n",
    "# only select a row if they are in the list\n",
    "top_ten_training = training_data.loc[training_data['club'].isin(top_ten),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_training.to_csv('../Data/training_data_total.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PSTAT_234",
   "language": "python",
   "name": "pstat_234"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
